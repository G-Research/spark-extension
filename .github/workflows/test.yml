name: Test

on:
  workflow_call:
    inputs:
      spark-compat-version:
        description: Spark compatibility version, e.g. 3.4
        required: true
        type: string
      spark-version:
        description: Spark version, e.g. 3.4.0
        required: true
        type: string
      scala-compat-version:
        description: Scala compatibility version, e.g. 2.12
        required: true
        type: string
      scala-version:
        description: Scala version, e.g. 2.12.15
        required: true
        type: string
      python-version:
        description: Python version, e.g. 3.8
        required: true
        type: string

jobs:
  test:
    name: Test (Spark ${{ inputs.spark-version }} Scala ${{ inputs.scala-version }} Python ${{ inputs.python-version }})
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v3

      - name: Set versions in pom.xml
        run: |
          ./set-version.sh ${{ inputs.spark-version }} ${{ inputs.scala-version }}
          git diff

      - name: Fetch Binaries Artifact
        uses: actions/download-artifact@v3
        with:
          name: Binaries-${{ inputs.spark-version }}-${{ inputs.scala-version }}
          path: .

      - name: Cache Maven packages
        uses: actions/cache@v3
        with:
          path: ~/.m2/repository
          key: ${{ runner.os }}-mvn-build-${{ inputs.spark-version }}-${{ inputs.scala-version }}-${{ hashFiles('pom.xml') }}

      - name: Setup JDK 1.8
        uses: actions/setup-java@v3
        with:
          java-version: '8'
          distribution: 'zulu'

      - name: Cache Pip packages
        # pyspark is not available for snapshots or scala other than 2.12
        # we would have to compile spark from sources for this, not worth it
        if: inputs.scala-compat-version == '2.12' && ! contains(inputs.spark-version, 'SNAPSHOT')
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-test-${{ inputs.python-version }}-${{ hashFiles('requirements.txt') }}

      - name: Setup Python
        # pyspark is not available for snapshots or scala other than 2.12
        # we would have to compile spark from sources for this, not worth it
        if: inputs.scala-compat-version == '2.12' && ! contains(inputs.spark-version, 'SNAPSHOT')
        uses: actions/setup-python@v4
        with:
          python-version: ${{ inputs.python-version }}

      - name: Install Python dependencies
        # pyspark is not available for snapshots or scala other than 2.12
        # we would have to compile spark from sources for this, not worth it
        if: inputs.scala-compat-version == '2.12' && ! contains(inputs.spark-version, 'SNAPSHOT')
        run: |
          python -m pip install --upgrade pip
          pip install pypandoc
          pip install -r python/requirements-${{ inputs.spark-compat-version }}_${{ inputs.scala-compat-version }}.txt
          pip install pytest

      - name: Scala and Java Tests
        run: mvn --batch-mode test

      - name: Python Tests
        # pyspark is not available for snapshots or scala other than 2.12
        # we would have to compile spark from sources for this, not worth it
        if: inputs.scala-compat-version == '2.12' && ! contains(inputs.spark-version, 'SNAPSHOT')
        env:
          PYTHONPATH: python:python/test
        run: |
          mkdir -p target/surefire-reports
          python -m pytest python/test --junit-xml target/surefire-reports/pytest.xml

      - name: Generate Unit Test Report
        if: failure()
        run: mvn --batch-mode surefire-report:report-only

      - name: Upload Unit Test Results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: Unit Test Results (Spark ${{ inputs.spark-version }} Scala ${{ inputs.scala-version }} Python ${{ inputs.python-version }})
          path: |
            target/surefire-reports/*.xml
            !target/surefire-reports/TEST-org.scalatest*.xml
            target/site/surefire-report.html

