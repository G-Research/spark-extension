name: 'Test JVM'
author: 'EnricoMi'
description: 'A GitHub Action that tests JVM spark-extension'

inputs:
  spark-version:
    description: Spark version, e.g. 3.4.0, 3.4.0-SNAPSHOT or 4.0.0-preview1
    required: true
  spark-compat-version:
    description: Spark compatibility version, e.g. 3.4
    required: true
  spark-archive-url:
    description: The URL to download the Spark binary distribution
    required: false
  scala-version:
    description: Scala version, e.g. 2.12.15
    required: true
  scala-compat-version:
    description: Scala compatibility version, e.g. 2.12
    required: true
  hadoop-version:
    description: Hadoop version, e.g. 2.7 or 2
    required: true
  java-compat-version:
    description: Java compatibility version, e.g. 8
    required: true

runs:
  using: 'composite'
  steps:
  - name: Fetch Binaries Artifact
    uses: actions/download-artifact@v4
    with:
      name: Binaries-${{ inputs.spark-compat-version }}-${{ inputs.scala-compat-version }}
      path: .

  - name: Set versions in pom.xml
    run: |
      ./set-version.sh ${{ inputs.spark-version }} ${{ inputs.scala-version }}
      git diff
    shell: bash

  - name: Restore Spark Binaries cache
    if: github.event_name != 'schedule' && ! contains(inputs.spark-version, '-SNAPSHOT')
    uses: actions/cache/restore@v4
    with:
      path: ~/spark
      key: ${{ runner.os }}-spark-binaries-${{ inputs.spark-version }}-${{ inputs.scala-compat-version }}
      restore-keys: |
        ${{ runner.os }}-spark-binaries-${{ inputs.spark-version }}-${{ inputs.scala-compat-version }}

  - name: Setup Spark Binaries
    if: ( ! contains(inputs.spark-version, '-SNAPSHOT') )
    env:
      SPARK_PACKAGE: spark-${{ inputs.spark-version }}/spark-${{ inputs.spark-version }}-bin-hadoop${{ inputs.hadoop-version }}${{ startsWith(inputs.spark-version, '3.') && inputs.scala-compat-version == '2.13' && '-scala2.13' || '' }}.tgz
    run: |
      # Setup Spark Binaries
      if [[ ! -e ~/spark ]]
      then
        url="${{ inputs.spark-archive-url }}"
        wget --progress=dot:giga "${url:-https://www.apache.org/dyn/closer.lua/spark/${SPARK_PACKAGE}?action=download}" -O - | tar -xzC "${{ runner.temp }}"
        archive=$(basename "${SPARK_PACKAGE}") bash -c "mv -v "${{ runner.temp }}/\${archive/%.tgz/}" ~/spark"
      fi
      echo "SPARK_HOME=$(cd ~/spark; pwd)" >> $GITHUB_ENV
    shell: bash

  - name: Restore Maven packages cache
    if: github.event_name != 'schedule'
    uses: actions/cache/restore@v4
    with:
      path: ~/.m2/repository
      key: ${{ runner.os }}-mvn-build-${{ inputs.spark-version }}-${{ inputs.scala-version }}-${{ hashFiles('pom.xml') }}
      restore-keys: |
        ${{ runner.os }}-mvn-build-${{ inputs.spark-version }}-${{ inputs.scala-version }}-${{ hashFiles('pom.xml') }}
        ${{ runner.os }}-mvn-build-${{ inputs.spark-version }}-${{ inputs.scala-version }}-

  - name: Setup JDK ${{ inputs.java-compat-version }}
    uses: actions/setup-java@v4
    with:
      java-version: ${{ inputs.java-compat-version }}
      distribution: 'zulu'

  - name: Scala and Java Tests
    env:
      JDK_JAVA_OPTIONS: --add-exports java.base/sun.nio.ch=ALL-UNNAMED --add-exports java.base/sun.util.calendar=ALL-UNNAMED
    run: |
      # Scala and Java Tests
      echo "::group::mvn test"
      mvn --batch-mode --update-snapshots -Dspotless.check.skip test
      echo "::endgroup::"
    shell: bash

  - name: Generate Unit Test Report
    if: failure()
    run: |
      # Generate Unit Test Report
      echo "::group::mvn report-only"
      mvn --batch-mode surefire-report:report-only
      echo "::endgroup::"
    shell: bash

  - name: Upload Unit Test Results
    if: always()
    uses: actions/upload-artifact@v4
    with:
      name: JVM Test Results (Spark ${{ inputs.spark-version }} Scala ${{ inputs.scala-version }})
      path: |
        target/surefire-reports/*.xml
        !target/surefire-reports/TEST-org.scalatest*.xml
        target/site/surefire-report.html

branding:
  icon: 'check-circle'
  color: 'green'
